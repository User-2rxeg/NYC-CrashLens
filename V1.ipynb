{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/User-2rxeg/NYC-CrashLens/blob/Integration/V1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 429
        },
        "id": "cRKyweK6S4kp",
        "outputId": "3f98f1b8-5778-4acd-ce32-b321a5fc6f83"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   CRASH DATE CRASH TIME   BOROUGH ZIP CODE  LATITUDE  LONGITUDE  \\\n",
              "0  09/11/2021       2:39       NaN      NaN       NaN        NaN   \n",
              "1  03/26/2022      11:45       NaN      NaN       NaN        NaN   \n",
              "2  11/01/2023       1:29  BROOKLYN    11230  40.62179 -73.970024   \n",
              "3  06/29/2022       6:55       NaN      NaN       NaN        NaN   \n",
              "4  09/21/2022      13:21       NaN      NaN       NaN        NaN   \n",
              "\n",
              "                 LOCATION           ON STREET NAME CROSS STREET NAME  \\\n",
              "0                     NaN    WHITESTONE EXPRESSWAY         20 AVENUE   \n",
              "1                     NaN  QUEENSBORO BRIDGE UPPER               NaN   \n",
              "2  (40.62179, -73.970024)            OCEAN PARKWAY          AVENUE K   \n",
              "3                     NaN       THROGS NECK BRIDGE               NaN   \n",
              "4                     NaN          BROOKLYN BRIDGE               NaN   \n",
              "\n",
              "  OFF STREET NAME  ...  CONTRIBUTING FACTOR VEHICLE 2  \\\n",
              "0             NaN  ...                    Unspecified   \n",
              "1             NaN  ...                            NaN   \n",
              "2             NaN  ...                    Unspecified   \n",
              "3             NaN  ...                    Unspecified   \n",
              "4             NaN  ...                    Unspecified   \n",
              "\n",
              "   CONTRIBUTING FACTOR VEHICLE 3  CONTRIBUTING FACTOR VEHICLE 4  \\\n",
              "0                            NaN                            NaN   \n",
              "1                            NaN                            NaN   \n",
              "2                    Unspecified                            NaN   \n",
              "3                            NaN                            NaN   \n",
              "4                            NaN                            NaN   \n",
              "\n",
              "   CONTRIBUTING FACTOR VEHICLE 5  COLLISION_ID  \\\n",
              "0                            NaN       4455765   \n",
              "1                            NaN       4513547   \n",
              "2                            NaN       4675373   \n",
              "3                            NaN       4541903   \n",
              "4                            NaN       4566131   \n",
              "\n",
              "                   VEHICLE TYPE CODE 1  VEHICLE TYPE CODE 2  \\\n",
              "0                                Sedan                Sedan   \n",
              "1                                Sedan                  NaN   \n",
              "2                                Moped                Sedan   \n",
              "3                                Sedan        Pick-up Truck   \n",
              "4  Station Wagon/Sport Utility Vehicle                  NaN   \n",
              "\n",
              "   VEHICLE TYPE CODE 3 VEHICLE TYPE CODE 4 VEHICLE TYPE CODE 5  \n",
              "0                  NaN                 NaN                 NaN  \n",
              "1                  NaN                 NaN                 NaN  \n",
              "2                Sedan                 NaN                 NaN  \n",
              "3                  NaN                 NaN                 NaN  \n",
              "4                  NaN                 NaN                 NaN  \n",
              "\n",
              "[5 rows x 29 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-bba2d791-ee99-4bbf-842e-45cf5e27ccc3\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>CRASH DATE</th>\n",
              "      <th>CRASH TIME</th>\n",
              "      <th>BOROUGH</th>\n",
              "      <th>ZIP CODE</th>\n",
              "      <th>LATITUDE</th>\n",
              "      <th>LONGITUDE</th>\n",
              "      <th>LOCATION</th>\n",
              "      <th>ON STREET NAME</th>\n",
              "      <th>CROSS STREET NAME</th>\n",
              "      <th>OFF STREET NAME</th>\n",
              "      <th>...</th>\n",
              "      <th>CONTRIBUTING FACTOR VEHICLE 2</th>\n",
              "      <th>CONTRIBUTING FACTOR VEHICLE 3</th>\n",
              "      <th>CONTRIBUTING FACTOR VEHICLE 4</th>\n",
              "      <th>CONTRIBUTING FACTOR VEHICLE 5</th>\n",
              "      <th>COLLISION_ID</th>\n",
              "      <th>VEHICLE TYPE CODE 1</th>\n",
              "      <th>VEHICLE TYPE CODE 2</th>\n",
              "      <th>VEHICLE TYPE CODE 3</th>\n",
              "      <th>VEHICLE TYPE CODE 4</th>\n",
              "      <th>VEHICLE TYPE CODE 5</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>09/11/2021</td>\n",
              "      <td>2:39</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>WHITESTONE EXPRESSWAY</td>\n",
              "      <td>20 AVENUE</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>Unspecified</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>4455765</td>\n",
              "      <td>Sedan</td>\n",
              "      <td>Sedan</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>03/26/2022</td>\n",
              "      <td>11:45</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>QUEENSBORO BRIDGE UPPER</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>4513547</td>\n",
              "      <td>Sedan</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>11/01/2023</td>\n",
              "      <td>1:29</td>\n",
              "      <td>BROOKLYN</td>\n",
              "      <td>11230</td>\n",
              "      <td>40.62179</td>\n",
              "      <td>-73.970024</td>\n",
              "      <td>(40.62179, -73.970024)</td>\n",
              "      <td>OCEAN PARKWAY</td>\n",
              "      <td>AVENUE K</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>Unspecified</td>\n",
              "      <td>Unspecified</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>4675373</td>\n",
              "      <td>Moped</td>\n",
              "      <td>Sedan</td>\n",
              "      <td>Sedan</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>06/29/2022</td>\n",
              "      <td>6:55</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>THROGS NECK BRIDGE</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>Unspecified</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>4541903</td>\n",
              "      <td>Sedan</td>\n",
              "      <td>Pick-up Truck</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>09/21/2022</td>\n",
              "      <td>13:21</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>BROOKLYN BRIDGE</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>Unspecified</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>4566131</td>\n",
              "      <td>Station Wagon/Sport Utility Vehicle</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows Ã— 29 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-bba2d791-ee99-4bbf-842e-45cf5e27ccc3')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-bba2d791-ee99-4bbf-842e-45cf5e27ccc3 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-bba2d791-ee99-4bbf-842e-45cf5e27ccc3');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-bd268534-8be8-425d-8727-caa9544b231f\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-bd268534-8be8-425d-8727-caa9544b231f')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-bd268534-8be8-425d-8727-caa9544b231f button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df_crashes"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load crashes dataset\n",
        "crashes_url = 'https://data.cityofnewyork.us/api/views/h9gi-nx95/rows.csv?accessType=DOWNLOAD'\n",
        "df_crashes = pd.read_csv(crashes_url, low_memory=False)\n",
        "\n",
        "\n",
        "\n",
        "# Quick preview\n",
        "df_crashes.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "id": "2d5814a5",
        "outputId": "0fde7a42-70e3-4e7d-a736-d7aad3b12aa0"
      },
      "source": [
        "print(\"=== OPTIMIZING REMAINING DATA TYPES ===\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Convert CRASH_TIME to string type\n",
        "df_integrated['CRASH_TIME'] = df_integrated['CRASH_TIME'].astype(str)\n",
        "\n",
        "# Convert injury/killed counts and person age to Int64 (nullable integer type)\n",
        "for col in ['NUMBER_OF_PERSONS_INJURED', 'NUMBER_OF_PERSONS_KILLED', 'PERSON_AGE']:\n",
        "    if col in df_integrated.columns:\n",
        "        df_integrated[col] = df_integrated[col].astype('Int64')\n",
        "\n",
        "print(\"âœ… Data types optimized for CRASH_TIME, NUMBER_OF_PERSONS_INJURED, NUMBER_OF_PERSONS_KILLED, and PERSON_AGE.\")\n",
        "\n",
        "print(\"\\n=== df_integrated Data Types - After final optimization ===\")\n",
        "print(\"=\"*50)\n",
        "df_integrated.info(verbose=True, show_counts=True)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== OPTIMIZING REMAINING DATA TYPES ===\n",
            "==================================================\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'df_integrated' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3923269203.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Convert CRASH_TIME to string type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mdf_integrated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'CRASH_TIME'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_integrated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'CRASH_TIME'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Convert injury/killed counts and person age to Int64 (nullable integer type)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'df_integrated' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "05eac11b"
      },
      "source": [
        "# === Data Type Optimization ===\n",
        "\n",
        "print(\"=== Optimizing Data Types ===\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Convert CRASH_TIME to string type\n",
        "df_integrated['CRASH_TIME'] = df_integrated['CRASH_TIME'].astype(str)\n",
        "\n",
        "# Columns to convert to pd.CategoricalDtype()\n",
        "categorical_cols = [\n",
        "    'BOROUGH',\n",
        "    'CONTRIBUTING_FACTOR_VEHICLE_1',\n",
        "    'CONTRIBUTING_FACTOR_VEHICLE_2',\n",
        "    'CONTRIBUTING_FACTOR_VEHICLE_3',\n",
        "    'CONTRIBUTING_FACTOR_VEHICLE_4',\n",
        "    'CONTRIBUTING_FACTOR_VEHICLE_5',\n",
        "    'VEHICLE_TYPE_CODE_1',\n",
        "    'VEHICLE_TYPE_CODE_2',\n",
        "    'VEHICLE_TYPE_CODE_3',\n",
        "    'VEHICLE_TYPE_CODE_4',\n",
        "    'VEHICLE_TYPE_CODE_5',\n",
        "    'PERSON_TYPE',\n",
        "    'PERSON_INJURY',\n",
        "    'PERSON_SEX',\n",
        "    'EJECTION',\n",
        "    'EMOTIONAL_STATUS',\n",
        "    'BODILY_INJURY',\n",
        "    'POSITION_IN_VEHICLE',\n",
        "    'SAFETY_EQUIPMENT',\n",
        "    'PED_LOCATION',\n",
        "    'PED_ACTION',\n",
        "    'COMPLAINT',\n",
        "    'PED_ROLE',\n",
        "    'CONTRIBUTING_FACTOR_1',\n",
        "    'CONTRIBUTING_FACTOR_2'\n",
        "]\n",
        "\n",
        "for col in categorical_cols:\n",
        "    if col in df_integrated.columns:\n",
        "        df_integrated[col] = df_integrated[col].astype('category')\n",
        "\n",
        "# Columns to convert to pd.StringDtype() for text representation\n",
        "string_cols = [\n",
        "    'ZIP_CODE',\n",
        "    'LOCATION',\n",
        "    'ON_STREET_NAME',\n",
        "    'CROSS_STREET_NAME',\n",
        "    'OFF_STREET_NAME'\n",
        "]\n",
        "\n",
        "for col in string_cols:\n",
        "    if col in df_integrated.columns:\n",
        "        df_integrated[col] = df_integrated[col].astype(pd.StringDtype())\n",
        "\n",
        "print(\"âœ… Data types optimized for better performance and consistency.\")\n",
        "\n",
        "print(\"\\n=== df_integrated Data Types - After Optimization ===\")\n",
        "print(\"=\"*50)\n",
        "df_integrated.info(verbose=True, show_counts=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bf725019"
      },
      "source": [
        "# === Data Type Optimization ===\n",
        "\n",
        "print(\"=== Optimizing Data Types ===\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Convert CRASH_TIME to string type\n",
        "df_integrated['CRASH_TIME'] = df_integrated['CRASH_TIME'].astype(str)\n",
        "\n",
        "# Columns to convert to pd.CategoricalDtype()\n",
        "categorical_cols = [\n",
        "    'BOROUGH',\n",
        "    'CONTRIBUTING_FACTOR_VEHICLE_1',\n",
        "    'CONTRIBUTING_FACTOR_VEHICLE_2',\n",
        "    'CONTRIBUTING_FACTOR_VEHICLE_3',\n",
        "    'CONTRIBUTING_FACTOR_VEHICLE_4',\n",
        "    'CONTRIBUTING_FACTOR_VEHICLE_5',\n",
        "    'VEHICLE_TYPE_CODE_1',\n",
        "    'VEHICLE_TYPE_CODE_2',\n",
        "    'VEHICLE_TYPE_CODE_3',\n",
        "    'VEHICLE_TYPE_CODE_4',\n",
        "    'VEHICLE_TYPE_CODE_5',\n",
        "    'PERSON_TYPE',\n",
        "    'PERSON_INJURY',\n",
        "    'PERSON_SEX',\n",
        "    'EJECTION',\n",
        "    'EMOTIONAL_STATUS',\n",
        "    'BODILY_INJURY',\n",
        "    'POSITION_IN_VEHICLE',\n",
        "    'SAFETY_EQUIPMENT',\n",
        "    'PED_LOCATION',\n",
        "    'PED_ACTION',\n",
        "    'COMPLAINT',\n",
        "    'PED_ROLE',\n",
        "    'CONTRIBUTING_FACTOR_1',\n",
        "    'CONTRIBUTING_FACTOR_2'\n",
        "]\n",
        "\n",
        "for col in categorical_cols:\n",
        "    if col in df_integrated.columns:\n",
        "        df_integrated[col] = df_integrated[col].astype('category')\n",
        "\n",
        "# Columns to convert to pd.StringDtype() for text representation\n",
        "string_cols = [\n",
        "    'ZIP_CODE',\n",
        "    'LOCATION',\n",
        "    'ON_STREET_NAME',\n",
        "    'CROSS_STREET_NAME',\n",
        "    'OFF_STREET_NAME'\n",
        "]\n",
        "\n",
        "for col in string_cols:\n",
        "    if col in df_integrated.columns:\n",
        "        df_integrated[col] = df_integrated[col].astype(pd.StringDtype())\n",
        "\n",
        "print(\"âœ… Data types optimized for better performance and consistency.\")\n",
        "\n",
        "print(\"\\n=== df_integrated Data Types - After Optimization ===\")\n",
        "print(\"=\"*50)\n",
        "df_integrated.info(verbose=True, show_counts=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4552498b"
      },
      "source": [
        "df_integrated['PERSON_AGE'] = pd.to_numeric(df_integrated['PERSON_AGE'], errors='coerce')\n",
        "\n",
        "print(\"Data type for 'PERSON_AGE' updated to numeric.\")\n",
        "\n",
        "print(\"\\n=== df_integrated Data Types - After final optimization ===\")\n",
        "print(\"=\"*50)\n",
        "df_integrated.info(verbose=True, show_counts=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "52425809"
      },
      "source": [
        "df_integrated['PERSON_AGE'] = pd.to_numeric(df_integrated['PERSON_AGE'], errors='coerce')\n",
        "\n",
        "print(\"Data type for 'PERSON_AGE' updated to numeric.\")\n",
        "\n",
        "print(\"\\n=== df_integrated Data Types - After final optimization ===\")\n",
        "print(\"=\"*50)\n",
        "df_integrated.info(verbose=True, show_counts=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "777287b7"
      },
      "source": [
        "print(\"=== POST-INTEGRATION CLEANING VERIFICATION ===\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "print(\"\\nðŸ“‹ Sample of df_integrated after cleaning:\")\n",
        "display(df_integrated[['COLLISION_ID', 'CRASH_DATE', 'CRASH_TIME', 'BOROUGH', 'PERSON_ID', 'PERSON_TYPE', 'PERSON_AGE', 'HAS_PERSON_DATA']].head())\n",
        "\n",
        "print(\"\\nâœ… Confirmed: Column conflicts and new missing values have been handled as requested.\")\n",
        "print(\"   - Redundant columns have been removed/renamed.\")\n",
        "print(\"   - Person-specific missing values for non-associated crashes are marked (e.g., 'NO_PERSON_RECORD' or -1).\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ec029a2"
      },
      "source": [
        "print(\"=== df_integrated Data Types - Initial Check ===\")\n",
        "print(\"=\"*50)\n",
        "df_integrated.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kzsxZiLSCcO0"
      },
      "outputs": [],
      "source": [
        "# Load persons dataset\n",
        "persons_url = 'https://data.cityofnewyork.us/api/views/f55k-p6yu/rows.csv?accessType=download'\n",
        "df_persons = pd.read_csv(persons_url, low_memory=False)\n",
        "\n",
        "df_persons.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2rQI_iUi-LXA"
      },
      "outputs": [],
      "source": [
        "print(\"Crashes Dataset Structure:\")\n",
        "df_crashes.info()\n",
        "df_crashes.shape\n",
        "\n",
        "print(\"\\nPersons Dataset Structure:\")\n",
        "df_persons.info()\n",
        "df_persons.shape\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9OgxrnbUS8kl"
      },
      "outputs": [],
      "source": [
        "# Count missing values in each dataset\n",
        "print(\"Missing values in Crashes dataset:\")\n",
        "print(df_crashes.isna().sum().sort_values(ascending=False).head(15))\n",
        "\n",
        "print(\"\\nMissing values in Persons dataset:\")\n",
        "print(df_persons.isna().sum().sort_values(ascending=False).head(15))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1rDwWQ3rVvR_"
      },
      "outputs": [],
      "source": [
        "df_persons.describe()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6eo-WqscXgeb"
      },
      "outputs": [],
      "source": [
        "df_crashes.describe()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v4OeUry1V1Dl"
      },
      "outputs": [],
      "source": [
        "duplicates_crashes = df_crashes.duplicated(subset=[\"COLLISION_ID\"]).sum()\n",
        "duplicates_persons = df_persons.duplicated(subset=[\"COLLISION_ID\"]).sum()\n",
        "\n",
        "print(\"Duplicate collision IDs in crashes:\", duplicates_crashes)\n",
        "print(\"Duplicate collision IDs in persons:\", duplicates_persons)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "85IMhHG8X6ZT"
      },
      "outputs": [],
      "source": [
        "#  Standardize formats Date:\n",
        "df_crashes['CRASH DATE'] = pd.to_datetime(df_crashes['CRASH DATE'], errors='coerce')\n",
        "df_crashes['YEAR'] = df_crashes['CRASH DATE'].dt.year\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F_1oZtYeZDIM"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "df_crashes['YEAR'].value_counts().sort_index().plot(kind='bar')\n",
        "plt.title(\"Number of crashes per year\")\n",
        "plt.xlabel(\"Year\")\n",
        "plt.ylabel(\"Number of crashes\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lszrz3NEZWpw"
      },
      "outputs": [],
      "source": [
        "df_crashes['BOROUGH'].value_counts().plot(kind='bar')\n",
        "plt.title(\"Crashes count per Borough\")\n",
        "plt.xlabel(\"Borough\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eAKoVRHuZX1e"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "\n",
        "sns.heatmap(df_crashes[['NUMBER OF PERSONS INJURED',\n",
        "                        'NUMBER OF PERSONS KILLED',\n",
        "                        'NUMBER OF PEDESTRIANS INJURED',\n",
        "                        'NUMBER OF PEDESTRIANS KILLED']].corr(),\n",
        "            annot=True, cmap='Blues')\n",
        "plt.title(\"Correlation between injuries & fatalities\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9eOky_LBmCrc"
      },
      "outputs": [],
      "source": [
        "# âœ… 1) Standardize Column Names\n",
        "# -----------------------------\n",
        "df_crashes.columns = df_crashes.columns.str.replace(' ', '_')\n",
        "df_persons.columns = df_persons.columns.str.replace(' ', '_')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cS014sIPv_0Z"
      },
      "outputs": [],
      "source": [
        "string_cols_crashes = [\n",
        "    'BOROUGH',\n",
        "    'CONTRIBUTING FACTOR VEHICLE 1',\n",
        "    'VEHICLE TYPE CODE 1'\n",
        "]\n",
        "\n",
        "\n",
        "for col in string_cols_crashes:\n",
        "    if col in df_crashes.columns:\n",
        "        df_crashes[col] = df_crashes[col].astype(str).str.upper().str.strip()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QWtlA_Dsv-tP"
      },
      "outputs": [],
      "source": [
        "string_cols_persons = [\n",
        "    'PERSON_TYPE',\n",
        "    'PERSON_INJURY',\n",
        "    'PERSON_SEX',\n",
        "    'CONTRIBUTING_FACTOR_1',\n",
        "    'CONTRIBUTING_FACTOR_2'\n",
        "]\n",
        "\n",
        "for col in string_cols_persons:\n",
        "    if col in df_persons.columns:\n",
        "        df_persons[col] = df_persons[col].astype(str).str.upper().str.strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lLK6osud2QhL"
      },
      "outputs": [],
      "source": [
        "categorical_cols_crashes = [\n",
        "    'BOROUGH',\n",
        "    'CONTRIBUTING FACTOR VEHICLE 1',\n",
        "    'VEHICLE TYPE CODE 1'\n",
        "]\n",
        "\n",
        "for col in categorical_cols_crashes:\n",
        "    if col in df_crashes.columns:\n",
        "        df_crashes[col] = df_crashes[col].astype('category')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aQ8jDNoq2Ua2"
      },
      "outputs": [],
      "source": [
        "categorical_cols_persons = [\n",
        "    'PERSON_TYPE',\n",
        "    'PERSON_INJURY',\n",
        "    'PERSON_SEX',\n",
        "    'CONTRIBUTING_FACTOR_1'\n",
        "]\n",
        "\n",
        "for col in categorical_cols_persons:\n",
        "    if col in df_persons.columns:\n",
        "        df_persons[col] = df_persons[col].astype('category')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xe_XBuVeba1Z"
      },
      "outputs": [],
      "source": [
        "print(\" Crashes dataset â€” Before removing duplicates:\", len(df_crashes))\n",
        "\n",
        "df_crashes = df_crashes.drop_duplicates(subset=[\"COLLISION_ID\"])\n",
        "\n",
        "print(\" Crashes dataset â€” After removing duplicates:\", len(df_crashes))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PdgXm9eocxP9"
      },
      "outputs": [],
      "source": [
        "print(\"Persons dataset â€” before removing duplicates:\", len(df_persons))\n",
        "df_persons = df_persons.drop_duplicates(subset=[\"COLLISION_ID\"])\n",
        "print(\"Persons dataset â€” after removing duplicates:\", len(df_persons))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "97560bc7"
      },
      "source": [
        "## Verification: Outlier Check (Statistical - IQR Method)\n",
        "\n",
        "This section performs a statistical check for outliers using the Interquartile Range (IQR) method for all numerical columns in both `df_crashes` and `df_persons`. This is a general statistical method that identifies values falling significantly outside the typical range, and may include both genuinely erroneous data points and valid but extreme observations.\n",
        "\n",
        "It's important to differentiate this from our previous *domain-rule-based* outlier handling, which specifically targeted clearly impossible values (e.g., negative ages, coordinates outside NYC bounds) rather than just statistically unusual ones. This verification step helps confirm if other types of statistical outliers exist that might warrant further investigation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1fc7439c"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "print(\"### Statistical Outlier Check (IQR Method) ###\")\n",
        "\n",
        "def find_iqr_outliers(df, column):\n",
        "    Q1 = df[column].quantile(0.25)\n",
        "    Q3 = df[column].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    lower_bound = Q1 - 1.5 * IQR\n",
        "    upper_bound = Q3 + 1.5 * IQR\n",
        "    outliers = df[(df[column] < lower_bound) | (df[column] > upper_bound)]\n",
        "    return outliers.shape[0]\n",
        "\n",
        "# Check df_crashes for outliers\n",
        "print(\"\\n--- df_crashes Outliers (IQR) ---\")\n",
        "for col in df_crashes.select_dtypes(include=np.number).columns:\n",
        "    # Exclude IDs and all-NaN columns, and specific columns we already handled by domain rules if not already part of the general numeric selection.\n",
        "    # Note: LATITUDE/LONGITUDE will still be checked by IQR, but we've already handled the most egregious errors.\n",
        "    if col not in ['COLLISION_ID'] and not df_crashes[col].isnull().all():\n",
        "        num_outliers = find_iqr_outliers(df_crashes, col)\n",
        "        if num_outliers > 0:\n",
        "            print(f\"Column '{col}': {num_outliers} outliers detected\")\n",
        "\n",
        "# Check df_persons for outliers\n",
        "print(\"\\n--- df_persons Outliers (IQR) ---\")\n",
        "for col in df_persons.select_dtypes(include=np.number).columns:\n",
        "    # Exclude IDs and all-NaN columns\n",
        "    if col not in ['UNIQUE_ID', 'COLLISION_ID', 'VEHICLE_ID'] and not df_persons[col].isnull().all():\n",
        "        num_outliers = find_iqr_outliers(df_persons, col)\n",
        "        if num_outliers > 0:\n",
        "            print(f\"Column '{col}': {num_outliers} outliers detected\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dbfa815a"
      },
      "source": [
        "## Verification: Missing Values Re-check\n",
        "\n",
        "This section re-checks the count and percentage of missing values for all columns in both `df_crashes` and `df_persons` after the cleaning steps. This serves as a final verification that the missing value handling was applied correctly and to identify any remaining missing data that was intentionally left as `NaN`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a8b74f7f"
      },
      "outputs": [],
      "source": [
        "# Handling outliers in df_persons['PERSON_AGE']\n",
        "df_persons['PERSON_AGE'] = df_persons['PERSON_AGE'].apply(lambda x: x if pd.isna(x) or (0 <= x <= 100) else pd.NA)\n",
        "\n",
        "# Handling outliers in df_crashes['LATITUDE'] and df_crashes['LONGITUDE']\n",
        "# Approximate NYC bounds\n",
        "min_lat, max_lat = 40.5, 40.9\n",
        "min_lon, max_lon = -74.25, -73.7\n",
        "\n",
        "df_crashes.loc[(df_crashes['LATITUDE'] == 0) |\n",
        "               (df_crashes['LATITUDE'] < min_lat) |\n",
        "               (df_crashes['LATITUDE'] > max_lat), 'LATITUDE'] = pd.NA\n",
        "\n",
        "df_crashes.loc[(df_crashes['LONGITUDE'] == 0) |\n",
        "               (df_crashes['LONGITUDE'] < min_lon) |\n",
        "               (df_crashes['LONGITUDE'] > max_lon), 'LONGITUDE'] = pd.NA\n",
        "\n",
        "print(\"Outlier handling complete for PERSON_AGE, LATITUDE, and LONGITUDE.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8a0c5a4c"
      },
      "outputs": [],
      "source": [
        "print(\"### Missing Values Re-check ###\")\n",
        "\n",
        "print(\"\\n--- df_crashes Missing Values (Post-Cleaning) ---\")\n",
        "missing_crashes = df_crashes.isna().sum()\n",
        "missing_crashes = missing_crashes[missing_crashes > 0].sort_values(ascending=False)\n",
        "missing_crashes_percent = (missing_crashes / len(df_crashes)) * 100\n",
        "if not missing_crashes.empty:\n",
        "    missing_crashes_df = pd.DataFrame({'Missing Count': missing_crashes, 'Missing Percent': missing_crashes_percent.round(2)})\n",
        "    display(missing_crashes_df)\n",
        "else:\n",
        "    print(\"No missing values in df_crashes.\")\n",
        "\n",
        "print(\"\\n--- df_persons Missing Values (Post-Cleaning) ---\")\n",
        "missing_persons = df_persons.isna().sum()\n",
        "missing_persons = missing_persons[missing_persons > 0].sort_values(ascending=False)\n",
        "missing_persons_percent = (missing_persons / len(df_persons)) * 100\n",
        "if not missing_persons.empty:\n",
        "    missing_persons_df = pd.DataFrame({'Missing Count': missing_persons, 'Missing Percent': missing_persons_percent.round(2)})\n",
        "    display(missing_persons_df)\n",
        "else:\n",
        "    print(\"No missing values in df_persons.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ff7bf922"
      },
      "source": [
        "## Task: Outlier Handling\n",
        "\n",
        "For `PERSON_AGE` in `df_persons`, outliers were identified as values less than 0 or greater than 100, as these are biologically impossible or highly improbable. These values were replaced with `pd.NA`.\n",
        "\n",
        "For `LATITUDE` and `LONGITUDE` in `df_crashes`, outliers were defined as values of 0 or coordinates falling outside the approximate geographical bounds of New York City. This approach uses domain-specific knowledge to identify erroneous entries rather than statistical methods like IQR, which might misinterpret valid extreme coordinates as outliers or fail to capture clearly incorrect values like 0.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c86a4551"
      },
      "source": [
        "## Task: Missing Value Handling\n",
        "\n",
        "**For `df_crashes`:**\n",
        "\n",
        "-   **Imputation (Mode):** For `VEHICLE_TYPE_CODE_1` and `CONTRIBUTING_FACTOR_VEHICLE_1`, missing values were imputed with the mode. These columns had a relatively small percentage of missing data (less than 1%), making mode imputation a reasonable strategy to preserve records without significantly distorting the distribution.\n",
        "-   **Imputation (Zero):** For `NUMBER_OF_PERSONS_INJURED` and `NUMBER_OF_PERSONS_KILLED`, missing values were imputed with `0`. As these are count columns, a missing value is often indicative of zero occurrences, especially when the missing percentage is very low.\n",
        "-   **Leaving as NaN:** Columns with a very high percentage of missing values (e.g., `VEHICLE_TYPE_CODE_X` for X=2,3,4,5, `CONTRIBUTING_FACTOR_VEHICLE_X` for X=2,3,4,5, `OFF_STREET_NAME`, `CROSS_STREET_NAME`, `ZIP_CODE`, `BOROUGH`, `ON_STREET_NAME`, `LOCATION`, `LATITUDE`, `LONGITUDE`) were left as `NaN`. Imputing these would be highly speculative and could introduce significant bias or inaccurate information, given the large proportion of missing entries. Dropping these rows would result in a substantial loss of valuable data.\n",
        "\n",
        "**For `df_persons`:**\n",
        "\n",
        "-   **Dropping Rows:** Rows with missing `PERSON_ID` were dropped. `PERSON_ID` is a unique identifier, and only 2 rows out of millions were missing this crucial piece of information. These rows were likely corrupted, and dropping them had a negligible impact on the dataset size.\n",
        "-   **Leaving as NaN:** For other columns with high percentages of missing values (e.g., `PED_ACTION`, `PED_LOCATION`, `SAFETY_EQUIPMENT`, `EJECTION`, `POSITION_IN_VEHICLE`, `EMOTIONAL_STATUS`, `BODILY_INJURY`, `COMPLAINT`, `PED_ROLE`, `PERSON_AGE`, `VEHICLE_ID`), values were left as `NaN`. Many of these fields are descriptive details that are often not recorded if they are not applicable or unknown. Imputing these without strong domain expertise or a more sophisticated approach could lead to incorrect assumptions and bias in the data. Leaving them as `NaN` is the most conservative approach to avoid introducing erroneous information."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2a9df574"
      },
      "outputs": [],
      "source": [
        "# --- Handling Missing Values in df_crashes ---\n",
        "\n",
        "for col in ['VEHICLE_TYPE_CODE_1', 'CONTRIBUTING_FACTOR_VEHICLE_1']:\n",
        "    if col in df_crashes.columns:\n",
        "        mode_value = df_crashes[col].mode()[0]\n",
        "        df_crashes[col].fillna(mode_value, inplace=True)\n",
        "\n",
        "\n",
        "for col in ['NUMBER_OF_PERSONS_INJURED', 'NUMBER_OF_PERSONS_KILLED']:\n",
        "    if col in df_crashes.columns:\n",
        "        df_crashes[col].fillna(0, inplace=True)\n",
        "\n",
        "\n",
        "print(\"Missing values handled for df_crashes (mode imputation for specific categoricals, 0 imputation for counts).\")\n",
        "\n",
        "# --- Handling Missing Values in df_persons ---\n",
        "\n",
        "df_persons.dropna(subset=['PERSON_ID'], inplace=True)\n",
        "\n",
        "\n",
        "\n",
        "print(\"Missing values handled for df_persons (rows with missing PERSON_ID dropped).\")\n",
        "\n",
        "\n",
        "# Display remaining missing values to verify\n",
        "print(\"\\nRemaining Missing values in Crashes dataset:\")\n",
        "print(df_crashes.isna().sum().sort_values(ascending=False).head(20))\n",
        "\n",
        "print(\"\\nRemaining Missing values in Persons dataset:\")\n",
        "print(df_persons.isna().sum().sort_values(ascending=False).head(20))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "538ba59e"
      },
      "outputs": [],
      "source": [
        "print(\"Missing values in Crashes dataset:\")\n",
        "print(df_crashes.isna().sum().sort_values(ascending=False).head(20))\n",
        "\n",
        "print(\"\\nMissing values in Persons dataset:\")\n",
        "print(df_persons.isna().sum().sort_values(ascending=False).head(20))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7qwmvjTq2L_p"
      },
      "source": [
        "## ðŸ”— Data Integration: Merging Crashes and Persons Datasets\n",
        "\n",
        "This section integrates the cleaned `df_crashes` and `df_persons` datasets using `COLLISION_ID` as the common key. The integration enriches crash records with detailed person-level information including demographics, injury types, and contributing factors from the person's perspective.\n",
        "\n",
        "### Integration Strategy Analysis\n",
        "\n",
        "Before performing the merge, let's analyze the relationship between the datasets to choose the appropriate join type."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cBqzPZnZ2L_p"
      },
      "outputs": [],
      "source": [
        "print(\"=== PRE-INTEGRATION ANALYSIS ===\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Analyze the relationship between datasets\n",
        "print(\"\\nðŸ“Š Dataset Overview:\")\n",
        "print(f\"   Crashes dataset: {len(df_crashes):,} records\")\n",
        "print(f\"   Persons dataset: {len(df_persons):,} records\")\n",
        "\n",
        "print(f\"\\nðŸ”‘ COLLISION_ID Analysis:\")\n",
        "print(f\"   Unique COLLISION_IDs in crashes: {df_crashes['COLLISION_ID'].nunique():,}\")\n",
        "print(f\"   Unique COLLISION_IDs in persons: {df_persons['COLLISION_ID'].nunique():,}\")\n",
        "\n",
        "# Check overlap between datasets\n",
        "crashes_ids = set(df_crashes['COLLISION_ID'])\n",
        "persons_ids = set(df_persons['COLLISION_ID'])\n",
        "common_ids = crashes_ids.intersection(persons_ids)\n",
        "\n",
        "print(f\"\\nðŸ”„ ID Overlap Analysis:\")\n",
        "print(f\"   Common COLLISION_IDs: {len(common_ids):,}\")\n",
        "print(f\"   Crashes without person data: {len(crashes_ids - persons_ids):,}\")\n",
        "print(f\"   Persons without crash data: {len(persons_ids - crashes_ids):,}\")\n",
        "\n",
        "# Analyze one-to-many relationship\n",
        "sample_collision = df_persons['COLLISION_ID'].value_counts().head(1)\n",
        "print(f\"\\nðŸ“ˆ Relationship Analysis:\")\n",
        "print(f\"   Example: COLLISION_ID {sample_collision.index[0]} has {sample_collision.iloc[0]} person records\")\n",
        "print(f\"   Average persons per crash: {len(df_persons) / df_persons['COLLISION_ID'].nunique():.2f}\")\n",
        "\n",
        "# Check for potential data quality issues\n",
        "print(f\"\\nâš ï¸  Data Quality Check:\")\n",
        "print(f\"   Crashes with null COLLISION_ID: {df_crashes['COLLISION_ID'].isnull().sum()}\")\n",
        "print(f\"   Persons with null COLLISION_ID: {df_persons['COLLISION_ID'].isnull().sum()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o4oa1Wl-2L_p"
      },
      "source": [
        "### Join Type Selection and Justification\n",
        "\n",
        "**Available Join Options:**\n",
        "1. **INNER JOIN**: Keep only crashes that have person data\n",
        "2. **LEFT JOIN**: Keep all crashes, add person data where available  \n",
        "3. **RIGHT JOIN**: Keep all persons, add crash data where available\n",
        "4. **OUTER JOIN**: Keep all records from both datasets\n",
        "\n",
        "**Selected Approach: LEFT JOIN with df_crashes as the left table**\n",
        "\n",
        "**Justification:**\n",
        "- **Preserves all crash records**: Every crash is valuable for spatial and temporal analysis\n",
        "- **Enriches with person data**: Adds demographic and injury details where available\n",
        "- **Maintains crash-centric perspective**: Crash is the primary event, persons are additional details\n",
        "- **Realistic data representation**: Not all crashes have person-level data recorded\n",
        "- **Flexibility for analysis**: Allows both crash-level and person-level analysis from single dataset\n",
        "\n",
        "**Why not other joins:**\n",
        "- **INNER JOIN**: Would lose ~40% of crash records without person data\n",
        "- **RIGHT JOIN**: Would lose crash context for person records  \n",
        "- **OUTER JOIN**: Would create unnecessary complexity with crashes having no context"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oYhSBO3E2L_q"
      },
      "outputs": [],
      "source": [
        "print(\"=== PERFORMING DATA INTEGRATION ===\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Perform LEFT JOIN to preserve all crash records while enriching with person data\n",
        "print(\"ðŸ”— Executing LEFT JOIN on COLLISION_ID...\")\n",
        "\n",
        "# Use merge with indicator to track data sources\n",
        "df_integrated = pd.merge(\n",
        "    df_crashes,\n",
        "    df_persons,\n",
        "    on='COLLISION_ID',\n",
        "    how='left',\n",
        "    suffixes=('_crash', '_person'),\n",
        "    indicator=True\n",
        ")\n",
        "\n",
        "print(f\"âœ… Integration completed successfully!\")\n",
        "print(f\"   Original crashes: {len(df_crashes):,}\")\n",
        "print(f\"   Original persons: {len(df_persons):,}\")\n",
        "print(f\"   Integrated records: {len(df_integrated):,}\")\n",
        "\n",
        "# Analyze the merge results\n",
        "merge_stats = df_integrated['_merge'].value_counts()\n",
        "print(f\"\\nðŸ“Š Merge Statistics:\")\n",
        "for category, count in merge_stats.items():\n",
        "    percentage = (count / len(df_integrated)) * 100\n",
        "    print(f\"   {category}: {count:,} records ({percentage:.1f}%)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AmnjzQeZ2L_q"
      },
      "outputs": [],
      "source": [
        "print(\"=== POST-INTEGRATION VERIFICATION ===\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Verify the integration results\n",
        "print(f\"ðŸ“‹ Dataset Structure After Integration:\")\n",
        "print(f\"   Total columns: {len(df_integrated.columns)}\")\n",
        "print(f\"   Memory usage: {df_integrated.memory_usage(deep=True).sum() / 1024**2:.1f} MB\")\n",
        "\n",
        "# Debug: Check actual column names to find the correct ones\n",
        "print(f\"\\nðŸ” Available columns containing key terms:\")\n",
        "key_terms = ['COLLISION', 'CRASH', 'DATE', 'BOROUGH', 'PERSON_TYPE', 'PERSON_INJURY', 'PERSON_AGE']\n",
        "available_columns = []\n",
        "for term in key_terms:\n",
        "    matching_cols = [col for col in df_integrated.columns if term in col.upper()]\n",
        "    if matching_cols:\n",
        "        print(f\"   {term}: {matching_cols}\")\n",
        "        available_columns.extend(matching_cols)\n",
        "\n",
        "# Show sample of integrated data with correct column names\n",
        "print(f\"\\nðŸ” Sample Integrated Data:\")\n",
        "sample_with_person = df_integrated[df_integrated['_merge'] == 'both'].head(2)\n",
        "\n",
        "# Debug: Find the crash date column\n",
        "crash_date_cols = [col for col in df_integrated.columns if 'CRASH' in col and 'DATE' in col]\n",
        "print(\"Crash date columns found:\", crash_date_cols)\n",
        "\n",
        "# Also check all columns to see the pattern\n",
        "print(\"\\nAll columns:\")\n",
        "print(df_integrated.columns.tolist())\n",
        "\n",
        "# Use the correct column names based on the debug output\n",
        "sample_columns = []\n",
        "for col in ['COLLISION_ID', 'CRASH_DATE_crash', 'BOROUGH', 'PERSON_TYPE', 'PERSON_INJURY', 'PERSON_AGE']:\n",
        "    if col in df_integrated.columns:\n",
        "        sample_columns.append(col)\n",
        "    else:\n",
        "        # Try alternative names for any missing columns\n",
        "        if col == 'CRASH_DATE_crash' and 'CRASH_DATE_crash' not in df_integrated.columns:\n",
        "            alternatives = [c for c in df_integrated.columns if 'CRASH' in c and 'DATE' in c]\n",
        "            if alternatives:\n",
        "                sample_columns.append(alternatives[0])\n",
        "                print(f\"   Note: Using {alternatives[0]} instead of {col}\")\n",
        "\n",
        "if sample_columns:\n",
        "    print(f\"Sample columns to display: {sample_columns}\")\n",
        "    display(sample_with_person[sample_columns])\n",
        "else:\n",
        "    # Fallback: show first few columns\n",
        "    print(\"Using fallback - showing first 6 columns:\")\n",
        "    display(sample_with_person.iloc[:, :6])\n",
        "\n",
        "# Verify one-to-many relationship\n",
        "collision_counts = df_integrated['COLLISION_ID'].value_counts()\n",
        "print(f\"\\nðŸ“ˆ Relationship Verification:\")\n",
        "print(f\"   Crashes with 1 person record: {(collision_counts == 1).sum():,}\")\n",
        "print(f\"   Crashes with multiple person records: {(collision_counts > 1).sum():,}\")\n",
        "print(f\"   Maximum persons in single crash: {collision_counts.max()}\")\n",
        "\n",
        "# Check for new issues introduced by the merge\n",
        "print(f\"\\nâš ï¸  New Issues Check:\")\n",
        "print(f\"   Duplicate COLLISION_IDs in result: {df_integrated.duplicated(subset=['COLLISION_ID']).sum():,}\")\n",
        "print(f\"   Expected behavior: One-to-many relationship creates multiple rows per crash\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i3j1mUiI2L_q"
      },
      "source": [
        "### Post-Integration Issues Identified\n",
        "\n",
        "The integration process has revealed several issues that need to be addressed:\n",
        "\n",
        "1. **Column Naming Conflicts**: Some columns exist in both datasets with different meanings\n",
        "2. **New Missing Values**: LEFT JOIN creates NaN values for crashes without person data  \n",
        "3. **Data Duplication**: One-to-many relationship means crash data is repeated for each person\n",
        "\n",
        "These issues will be addressed in the post-integration cleaning phase."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y9MuYdHq2L_q"
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "print(\"=== POST-INTEGRATION CLEANING: Column Conflicts ===\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "suffix_groups = defaultdict(dict)\n",
        "for column_name in df_integrated.columns:\n",
        "    if column_name.endswith('_crash'):\n",
        "        suffix_groups[column_name[:-6]][\"crash\"] = column_name\n",
        "    elif column_name.endswith('_person'):\n",
        "        suffix_groups[column_name[:-7]][\"person\"] = column_name\n",
        "\n",
        "columns_to_drop = []\n",
        "rename_map = {}\n",
        "conflict_log = []\n",
        "\n",
        "for base_name, locations in suffix_groups.items():\n",
        "    crash_col = locations.get('crash')\n",
        "    person_col = locations.get('person')\n",
        "    if crash_col and person_col:\n",
        "        conflict_log.append((base_name, crash_col, person_col))\n",
        "        rename_map[crash_col] = base_name\n",
        "        columns_to_drop.append(person_col)\n",
        "    elif crash_col:\n",
        "        rename_map[crash_col] = base_name\n",
        "    elif person_col:\n",
        "        rename_map[person_col] = f\"{base_name}_PERSONONLY\"\n",
        "\n",
        "if conflict_log:\n",
        "    print(\" Overlapping columns detected:\")\n",
        "    for base_name, crash_col, person_col in conflict_log:\n",
        "        print(f\"   {base_name}: keeping {crash_col}, dropping {person_col}\")\n",
        "else:\n",
        "    print(\" No overlapping columns detected.\")\n",
        "\n",
        "if rename_map:\n",
        "    df_integrated = df_integrated.rename(columns=rename_map)\n",
        "\n",
        "if columns_to_drop:\n",
        "    unique_drop_cols = list(dict.fromkeys(columns_to_drop))\n",
        "    df_integrated = df_integrated.drop(columns=unique_drop_cols)\n",
        "    dropped_count = len(unique_drop_cols)\n",
        "else:\n",
        "    dropped_count = 0\n",
        "\n",
        "print(f\" Canonicalized {len(rename_map)} column names and removed {dropped_count} redundant columns.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oRc0UHH72L_q"
      },
      "outputs": [],
      "source": [
        "print(\"=== POST-INTEGRATION CLEANING: Missing Values ===\")\n",
        "print(\"=\"*65)\n",
        "\n",
        "df_integrated['HAS_PERSON_DATA'] = df_integrated['_merge'] == 'both'\n",
        "person_counts = df_integrated['HAS_PERSON_DATA'].value_counts(dropna=False)\n",
        "\n",
        "print(\" Person data availability:\")\n",
        "for has_person, count in person_counts.items():\n",
        "    status = \"Has person rows\" if has_person else \"Crash only\"\n",
        "    print(f\"   {status}: {count:,} records ({(count / len(df_integrated)) * 100:.1f}%)\")\n",
        "\n",
        "no_person_mask = ~df_integrated['HAS_PERSON_DATA']\n",
        "person_identifier_cols = ['PERSON_ID', 'VEHICLE_ID', 'UNIQUE_ID']\n",
        "person_categorical_cols = [\n",
        "    'PERSON_TYPE', 'PERSON_INJURY', 'PERSON_SEX', 'EJECTION', 'EMOTIONAL_STATUS',\n",
        "    'BODILY_INJURY', 'POSITION_IN_VEHICLE', 'SAFETY_EQUIPMENT', 'PED_LOCATION',\n",
        "    'PED_ACTION', 'COMPLAINT', 'PED_ROLE', 'CONTRIBUTING_FACTOR_1', 'CONTRIBUTING_FACTOR_2'\n",
        "]\n",
        "person_numeric_cols = ['PERSON_AGE']\n",
        "\n",
        "no_person_label = 'NO_PERSON_RECORD'\n",
        "unknown_label = 'UNKNOWN_VALUE'\n",
        "numeric_no_person = -1\n",
        "\n",
        "for col in person_identifier_cols:\n",
        "    if col in df_integrated.columns:\n",
        "        df_integrated[col] = df_integrated[col].astype('string')\n",
        "        df_integrated.loc[no_person_mask, col] = no_person_label\n",
        "        df_integrated[col] = df_integrated[col].fillna(unknown_label)\n",
        "\n",
        "for col in person_categorical_cols:\n",
        "    if col in df_integrated.columns:\n",
        "        df_integrated.loc[no_person_mask, col] = no_person_label\n",
        "        df_integrated[col] = df_integrated[col].fillna(unknown_label)\n",
        "\n",
        "for col in person_numeric_cols:\n",
        "    if col in df_integrated.columns:\n",
        "        df_integrated[col] = pd.to_numeric(df_integrated[col], errors='coerce')\n",
        "        df_integrated.loc[no_person_mask, col] = numeric_no_person\n",
        "        df_integrated[col] = df_integrated[col].fillna(numeric_no_person).astype('Int64')\n",
        "\n",
        "if '_merge' in df_integrated.columns:\n",
        "    df_integrated = df_integrated.drop(columns=['_merge'])\n",
        "\n",
        "tracked_cols = [\n",
        "    col for col in (person_identifier_cols + person_categorical_cols + person_numeric_cols)\n",
        "    if col in df_integrated.columns\n",
        "]\n",
        "remaining_missing = df_integrated[tracked_cols].isna().sum()\n",
        "remaining_missing = remaining_missing[remaining_missing > 0]\n",
        "\n",
        "if remaining_missing.empty:\n",
        "    print(\" All join-induced person-level missing values resolved.\")\n",
        "else:\n",
        "    print(\" Remaining person column nulls (should represent genuine unknowns):\")\n",
        "    print(remaining_missing)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SZ49JiWO2L_r"
      },
      "outputs": [],
      "source": [
        "print(\"=== ANALYTICAL FEATURES CREATION ===\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Create useful analytical features\n",
        "print(\"ðŸ”§ Creating analytical features...\")\n",
        "\n",
        "# Count of persons involved in each crash\n",
        "df_integrated['PERSONS_IN_CRASH'] = df_integrated.groupby('COLLISION_ID')['COLLISION_ID'].transform('count')\n",
        "\n",
        "# Create easy filtering columns\n",
        "df_integrated['HAS_COORDINATES'] = ~(df_integrated['LATITUDE'].isna() | df_integrated['LONGITUDE'].isna())\n",
        "df_integrated['HAS_INJURIES'] = (df_integrated['NUMBER_OF_PERSONS_INJURED'] > 0) | (df_integrated['NUMBER_OF_PERSONS_KILLED'] > 0)\n",
        "\n",
        "print(\"âœ… Analytical features created:\")\n",
        "print(\"   - PERSONS_IN_CRASH: Count of persons per crash\")\n",
        "print(\"   - HAS_PERSON_DATA: Boolean flag for person data availability\")\n",
        "print(\"   - HAS_COORDINATES: Boolean flag for location data availability\")\n",
        "print(\"   - HAS_INJURIES: Boolean flag for crashes with casualties\")\n",
        "\n",
        "# Show summary of new features\n",
        "print(f\"\\nðŸ“Š Feature Summary:\")\n",
        "print(f\"   Average persons per crash: {df_integrated['PERSONS_IN_CRASH'].mean():.2f}\")\n",
        "print(f\"   Crashes with coordinates: {df_integrated['HAS_COORDINATES'].sum():,}\")\n",
        "print(f\"   Crashes with injuries/fatalities: {df_integrated['HAS_INJURIES'].sum():,}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CRg-FyP92L_r"
      },
      "source": [
        "### Integration Summary and Usage Guidelines\n",
        "\n",
        "**âœ… Integration Completed Successfully**\n",
        "\n",
        "The datasets have been successfully integrated using a LEFT JOIN on `COLLISION_ID`, creating a comprehensive dataset that combines crash-level and person-level information.\n",
        "\n",
        "**ðŸ“Š Final Dataset Characteristics:**\n",
        "- **Preserves all crash records**: Every original crash is maintained\n",
        "- **Enriched with person data**: Demographic and injury details added where available  \n",
        "- **One-to-many structure**: Multiple rows per crash when multiple persons involved\n",
        "- **Clean column structure**: Conflicts resolved, redundancies removed\n",
        "- **Analysis-ready features**: Flags and counts added for easy filtering\n",
        "\n",
        "**ðŸŽ¯ Usage Guidelines for Analysis:**\n",
        "\n",
        "1. **Crash-Level Analysis** (spatial, temporal, severity patterns):\n",
        "   ```python\n",
        "   df_crash_level = df_integrated.drop_duplicates(subset='COLLISION_ID')\n",
        "   ```\n",
        "\n",
        "2. **Person-Level Analysis** (demographics, injury types):\n",
        "   ```python\n",
        "   df_person_level = df_integrated[df_integrated['HAS_PERSON_DATA']]\n",
        "   ```\n",
        "\n",
        "3. **Combined Analysis** (person characteristics + crash context):\n",
        "   ```python\n",
        "   # Use full df_integrated dataset\n",
        "   ```\n",
        "\n",
        "**ðŸ“ˆ Data Quality Achievements:**\n",
        "- Zero data loss from original crash records\n",
        "- Maintained referential integrity through COLLISION_ID\n",
        "- Clear documentation of data availability through flags\n",
        "- Preserved both crash and person perspectives where available"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3YW8hovH2L_r"
      },
      "outputs": [],
      "source": [
        "print(\"=== FINAL INTEGRATION VERIFICATION ===\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Final comprehensive check\n",
        "print(f\"ðŸ“Š Final Dataset Overview:\")\n",
        "print(f\"   Total records: {len(df_integrated):,}\")\n",
        "print(f\"   Total columns: {len(df_integrated.columns)}\")\n",
        "print(f\"   Unique crashes: {df_integrated['COLLISION_ID'].nunique():,}\")\n",
        "print(f\"   Memory usage: {df_integrated.memory_usage(deep=True).sum() / 1024**2:.1f} MB\")\n",
        "\n",
        "# Verify no data loss\n",
        "original_crashes = len(df_crashes)\n",
        "integrated_unique_crashes = df_integrated['COLLISION_ID'].nunique()\n",
        "print(f\"\\nâœ… Data Integrity Check:\")\n",
        "print(f\"   Original crashes: {original_crashes:,}\")\n",
        "print(f\"   Unique crashes in integrated dataset: {integrated_unique_crashes:,}\")\n",
        "print(f\"   Data loss: {original_crashes - integrated_unique_crashes} (Expected: 0)\")\n",
        "\n",
        "# Show column categories\n",
        "crash_cols = [col for col in df_integrated.columns if not any(x in col.lower() for x in ['person', 'ped_', 'safety', 'ejection', 'position', 'emotional', 'bodily', 'complaint'])]\n",
        "person_cols = [col for col in df_integrated.columns if any(x in col.lower() for x in ['person', 'ped_', 'safety', 'ejection', 'position', 'emotional', 'bodily', 'complaint'])]\n",
        "feature_cols = ['HAS_PERSON_DATA', 'PERSONS_IN_CRASH', 'HAS_COORDINATES', 'HAS_INJURIES']\n",
        "\n",
        "print(f\"\\nðŸ“‹ Column Categories:\")\n",
        "print(f\"   Crash-related columns: {len(crash_cols)}\")\n",
        "print(f\"   Person-related columns: {len(person_cols)}\")\n",
        "print(f\"   Analytical features: {len(feature_cols)}\")\n",
        "\n",
        "print(f\"\\nðŸŽ‰ Integration completed successfully!\")\n",
        "print(f\"Dataset is ready for interactive visualization and analysis.\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "My Python Kernel",
      "language": "python",
      "name": "myenv"
    },
    "language_info": {
      "name": "python",
      "version": "3.14.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}